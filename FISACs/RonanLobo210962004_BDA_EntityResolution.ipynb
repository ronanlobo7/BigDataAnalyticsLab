{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "76ded996",
      "metadata": {
        "id": "76ded996"
      },
      "source": [
        "##                     ENTITY RESOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29002652",
      "metadata": {
        "id": "29002652"
      },
      "source": [
        "## PySpark and System Imports\n",
        "\n",
        "Import essential libraries for running Spark applications and managing system-related tasks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20c71e3",
      "metadata": {
        "id": "d20c71e3"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import os\n",
        "import sys\n",
        "from pyspark import SparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd7eee04",
      "metadata": {
        "id": "bd7eee04"
      },
      "source": [
        "## PySpark SparkSession Configuration\n",
        "\n",
        "Import the SparkSession class from PySpark. SparkSession is the entry point to programming Spark with the Dataset and DataFrame API. Create a SparkSession with specific configuration settings. The `config` method is used to set the `spark.driver.memory` property to \"16g\", allocating 16 gigabytes of memory to the driver program. The `appName` method sets the name of the Spark application to \"chapter_2\". The `getOrCreate` method either retrieves an existing SparkSession or creates a new one if it doesn't exist.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46596dc4",
      "metadata": {
        "id": "46596dc4"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_2').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f201ce7",
      "metadata": {
        "id": "4f201ce7"
      },
      "source": [
        "## Reading CSV Data with PySpark\n",
        "\n",
        "The code snippet reads a CSV file into a PySpark DataFrame using the `spark.read.csv` method:\n",
        "\n",
        "- `prev = spark.read.csv(\"data/linkage/donation/block_1/block_1.csv\")`: Reads the CSV file located at the specified path (\"data/linkage/donation/block_1/block_1.csv\") into a DataFrame named `prev`. The `read.csv` method automatically infers the schema from the CSV file.\n",
        "\n",
        "- `prev`: Displays the DataFrame `prev`, showing the contents of the CSV file as a tabular representation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c2a20b1",
      "metadata": {
        "id": "9c2a20b1"
      },
      "outputs": [],
      "source": [
        "prev = spark.read.csv(\"data/linkage/donation/block_1/block_1.csv\")\n",
        "prev"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30955707",
      "metadata": {
        "id": "30955707"
      },
      "source": [
        "## Displaying DataFrame Contents in PySpark\n",
        "\n",
        "The code snippet uses the `show` method to display the first two rows of the DataFrame `prev`:\n",
        "\n",
        "- `prev.show(2)`: Displays the first two rows of the DataFrame `prev`. The `show` method is used to show a specific number of rows from the DataFrame. In this case, `2` indicates that the first two rows of the DataFrame will be displayed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be4b87a",
      "metadata": {
        "id": "2be4b87a"
      },
      "outputs": [],
      "source": [
        "prev.show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d570a83c",
      "metadata": {
        "id": "d570a83c"
      },
      "source": [
        "## Reading and Parsing CSV Data with PySpark\n",
        "\n",
        "The code snippet reads and parses a CSV file into a PySpark DataFrame using various options:\n",
        "\n",
        "- `parsed = spark.read.option(\"header\", \"true\").option(\"nullValue\", \"?\").option(\"inferSchema\", \"true\").csv(\"data/linkage/donation/block_1/block_1.csv\")`: Reads the CSV file located at the specified path (\"data/linkage/donation/block_1/block_1.csv\") into a DataFrame named `parsed`. Several options are used to customize the parsing behavior:\n",
        "  - `header=\"true\"`: Specifies that the first row of the CSV file contains the header, which is used to name the columns of the DataFrame.\n",
        "  - `nullValue=\"?\"`: Specifies that the string \"?\" should be interpreted as a null value in the DataFrame.\n",
        "  - `inferSchema=\"true\"`: Specifies that the data types of columns should be inferred from the data in the CSV file.\n",
        "\n",
        "The `read.csv` method automatically infers the schema from the CSV file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acc44e90",
      "metadata": {
        "id": "acc44e90"
      },
      "outputs": [],
      "source": [
        "parsed = spark.read.option(\"header\", \"true\").option(\"nullValue\", \"?\").option(\"inferSchema\", \"true\").csv(\"data/linkage/donation/block_1/block_1.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "706aacc8",
      "metadata": {
        "id": "706aacc8"
      },
      "source": [
        "## Printing DataFrame Schema and Displaying Data in PySpark\n",
        "\n",
        "The code snippet prints the schema of a DataFrame and displays the first five rows of the DataFrame:\n",
        "\n",
        "- `parsed.printSchema()`: Prints the schema of the DataFrame `parsed`, showing the data types of each column.\n",
        "- `parsed.show(5)`: Displays the first five rows of the DataFrame `parsed`. The `show` method is used to display a specific number of rows from the DataFrame. In this case, `5` indicates that the first five rows will be displayed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e23a5a",
      "metadata": {
        "id": "c7e23a5a"
      },
      "outputs": [],
      "source": [
        "parsed.printSchema()\n",
        "parsed.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e85974f",
      "metadata": {
        "id": "0e85974f"
      },
      "source": [
        "## Counting Rows in a DataFrame with PySpark\n",
        "\n",
        "The code snippet counts the number of rows in the DataFrame `parsed`:\n",
        "\n",
        "- `parsed.count()`: Returns the number of rows in the DataFrame `parsed`. The `count` method is used to calculate the total number of rows in a DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ca1c72",
      "metadata": {
        "id": "41ca1c72"
      },
      "outputs": [],
      "source": [
        "parsed.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddaae133",
      "metadata": {
        "id": "ddaae133"
      },
      "source": [
        "## Caching DataFrame in PySpark\n",
        "\n",
        "The code snippet caches the DataFrame `parsed` in memory for faster access:\n",
        "\n",
        "- `parsed.cache()`: Caches the DataFrame `parsed` in memory. Caching a DataFrame allows subsequent operations on the DataFrame to be faster, as the data is stored in memory and can be accessed more quickly than reading from disk. This can be particularly useful when you plan to reuse the DataFrame in multiple operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef6517c7",
      "metadata": {
        "id": "ef6517c7"
      },
      "outputs": [],
      "source": [
        "parsed.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0abe40d",
      "metadata": {
        "id": "b0abe40d"
      },
      "source": [
        "## Grouping, Aggregating, and Ordering Data in PySpark\n",
        "\n",
        "The code snippet demonstrates how to group, aggregate, and order data in a PySpark DataFrame:\n",
        "\n",
        "- `from pyspark.sql.functions import col`: Imports the `col` function from `pyspark.sql.functions`. The `col` function is used to create a column expression that represents a column in a DataFrame.\n",
        "\n",
        "- `parsed.groupBy(\"is_match\").count()`: Groups the DataFrame `parsed` by the column \"is_match\" and calculates the count of each group.\n",
        "\n",
        "- `orderBy(col(\"count\").desc())`: Orders the result by the \"count\" column in descending order. The `desc` method is used to specify descending order.\n",
        "\n",
        "- `show()`: Displays the result of the grouping, aggregation, and ordering operations. The `show` method is used to display the resulting DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae9aae31",
      "metadata": {
        "id": "ae9aae31"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "parsed.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d94fe6b9",
      "metadata": {
        "id": "d94fe6b9"
      },
      "source": [
        "## Creating a Temporary View in PySpark\n",
        "\n",
        "The code snippet creates a temporary view named \"linkage\" from the DataFrame `parsed`:\n",
        "\n",
        "- `parsed.createOrReplaceTempView(\"linkage\")`: Creates a temporary view named \"linkage\" from the DataFrame `parsed`. A temporary view allows you to query the DataFrame using Spark SQL. This view is temporary and will be available only within the current Spark session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe15fd2",
      "metadata": {
        "id": "0fe15fd2"
      },
      "outputs": [],
      "source": [
        "parsed.createOrReplaceTempView(\"linkage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6423b5e1",
      "metadata": {
        "id": "6423b5e1"
      },
      "source": [
        "## Executing SQL Queries on a DataFrame in PySpark\n",
        "\n",
        "The code snippet executes a SQL query on the DataFrame `linkage` using Spark SQL:\n",
        "\n",
        "- `spark.sql(\"\"\"...\"\"\")`: Executes a SQL query using Spark SQL. The triple quotes (`\"\"\"`) are used to create a multi-line string that contains the SQL query.\n",
        "\n",
        "- The SQL query:\n",
        "  - `SELECT is_match, COUNT(*) cnt`: Selects the \"is_match\" column from the \"linkage\" table and calculates the count of rows for each value of \"is_match\". The result column is named \"cnt\".\n",
        "  - `FROM linkage`: Specifies the \"linkage\" table as the source of data for the query.\n",
        "  - `GROUP BY is_match`: Groups the rows by the \"is_match\" column.\n",
        "  - `ORDER BY cnt DESC`: Orders the result by the \"cnt\" column in descending order.\n",
        "\n",
        "- `show()`: Displays the result of the SQL query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f846dabd",
      "metadata": {
        "id": "f846dabd"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT is_match, COUNT(*) cnt\n",
        "FROM linkage\n",
        "GROUP BY is_match\n",
        "ORDER BY cnt DESC\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569a0353",
      "metadata": {
        "id": "569a0353"
      },
      "source": [
        "## Describing Data in a PySpark DataFrame\n",
        "\n",
        "The code snippet computes summary statistics for the DataFrame `parsed`:\n",
        "\n",
        "- `summary = parsed.describe()`: Computes summary statistics for each numerical column in the DataFrame `parsed` and stores the result in the DataFrame `summary`. The `describe` method calculates statistics like count, mean, standard deviation, min, and max for each numerical column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0c5861a",
      "metadata": {
        "id": "b0c5861a"
      },
      "outputs": [],
      "source": [
        "summary = parsed.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42bc9637",
      "metadata": {
        "id": "42bc9637"
      },
      "source": [
        "## Selecting Columns from Summary Statistics in PySpark\n",
        "\n",
        "The code snippet selects specific columns from the `summary` DataFrame to display:\n",
        "\n",
        "- `summary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\")`: Selects the columns \"summary\", \"cmp_fname_c1\", and \"cmp_fname_c2\" from the `summary` DataFrame. The `select` method is used to choose specific columns from a DataFrame.\n",
        "\n",
        "- `show()`: Displays the selected columns from the `summary` DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f01d3dfd",
      "metadata": {
        "id": "f01d3dfd"
      },
      "outputs": [],
      "source": [
        "summary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0434fc6a",
      "metadata": {
        "id": "0434fc6a"
      },
      "source": [
        "## Filtering Data and Computing Summary Statistics in PySpark\n",
        "\n",
        "The code snippet filters the DataFrame `parsed` to separate matches and misses based on the \"is_match\" column, and then computes summary statistics for each:\n",
        "\n",
        "- `matches = parsed.where(\"is_match = true\")`: Filters the DataFrame `parsed` to include only rows where the \"is_match\" column is true. The `where` method is used to filter rows based on a condition specified in SQL syntax.\n",
        "\n",
        "- `match_summary = matches.describe()`: Computes summary statistics for each numerical column in the `matches` DataFrame and stores the result in the DataFrame `match_summary`.\n",
        "\n",
        "- `misses = parsed.filter(col(\"is_match\") == False)`: Filters the DataFrame `parsed` to include only rows where the \"is_match\" column is false. The `filter` method is used to filter rows based on a condition specified using the `col` function.\n",
        "\n",
        "- `miss_summary = misses.describe()`: Computes summary statistics for each numerical column in the `misses` DataFrame and stores the result in the DataFrame `miss_summary`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94ba2734",
      "metadata": {
        "id": "94ba2734"
      },
      "outputs": [],
      "source": [
        "matches = parsed.where(\"is_match = true\")\n",
        "match_summary = matches.describe()\n",
        "misses = parsed.filter(col(\"is_match\") == False)\n",
        "miss_summary = misses.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11da33e7",
      "metadata": {
        "id": "11da33e7"
      },
      "source": [
        "## Converting PySpark DataFrame to Pandas DataFrame\n",
        "\n",
        "The code snippet converts a PySpark DataFrame `summary` to a Pandas DataFrame `summary_p`:\n",
        "\n",
        "- `summary_p = summary.toPandas()`: Converts the PySpark DataFrame `summary` to a Pandas DataFrame `summary_p`. The `toPandas` method is used to convert a PySpark DataFrame to a Pandas DataFrame, allowing for easier manipulation and analysis using Pandas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e27a30",
      "metadata": {
        "id": "c7e27a30"
      },
      "outputs": [],
      "source": [
        "summary_p = summary.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93e2e750",
      "metadata": {
        "id": "93e2e750"
      },
      "source": [
        "## Displaying Summary Statistics in Pandas DataFrame\n",
        "\n",
        "The code snippet displays the first few rows and shape of the Pandas DataFrame `summary_p`:\n",
        "\n",
        "- `summary_p.head()`: Displays the first few rows of the Pandas DataFrame `summary_p` using the `head` method.\n",
        "\n",
        "- `summary_p.shape`: Returns the dimensions (number of rows, number of columns) of the Pandas DataFrame `summary_p` using the `shape` attribute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf679997",
      "metadata": {
        "id": "cf679997"
      },
      "outputs": [],
      "source": [
        "summary_p.head()\n",
        "summary_p.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc183c63",
      "metadata": {
        "id": "cc183c63"
      },
      "source": [
        "## Manipulating Pandas DataFrame Structure\n",
        "\n",
        "The code snippet manipulates the structure of the Pandas DataFrame `summary_p`:\n",
        "\n",
        "- `summary_p = summary_p.set_index('summary').transpose().reset_index()`: Transposes the DataFrame `summary_p` and sets the \"summary\" column as the index. The `reset_index()` method resets the index to integers.\n",
        "\n",
        "- `summary_p = summary_p.rename(columns={'index':'field'})`: Renames the \"index\" column to \"field\" in the DataFrame `summary_p`.\n",
        "\n",
        "- `summary_p = summary_p.rename_axis(None, axis=1)`: Removes the axis name from the columns of the DataFrame `summary_p`.\n",
        "\n",
        "- `summary_p.shape`: Returns the dimensions (number of rows, number of columns) of the Pandas DataFrame `summary_p` after manipulation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5948a239",
      "metadata": {
        "id": "5948a239"
      },
      "outputs": [],
      "source": [
        "summary_p = summary_p.set_index('summary').transpose().reset_index()\n",
        "summary_p = summary_p.rename(columns={'index':'field'})\n",
        "summary_p = summary_p.rename_axis(None, axis=1)\n",
        "summary_p.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9fc9eb9",
      "metadata": {
        "id": "d9fc9eb9"
      },
      "source": [
        "## Creating a PySpark DataFrame from Pandas DataFrame\n",
        "\n",
        "The code snippet creates a PySpark DataFrame `summaryT` from the Pandas DataFrame `summary_p`:\n",
        "\n",
        "- `summaryT = spark.createDataFrame(summary_p)`: Creates a PySpark DataFrame `summaryT` from the Pandas DataFrame `summary_p`. The `createDataFrame` method is used to convert a Pandas DataFrame to a PySpark DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be6e5518",
      "metadata": {
        "id": "be6e5518"
      },
      "outputs": [],
      "source": [
        "summaryT = spark.createDataFrame(summary_p)\n",
        "summaryT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3e248f3",
      "metadata": {
        "id": "f3e248f3"
      },
      "source": [
        "## Printing Schema of PySpark DataFrame\n",
        "\n",
        "The code snippet prints the schema of the PySpark DataFrame `summaryT`:\n",
        "\n",
        "- `summaryT.printSchema()`: Prints the schema of the PySpark DataFrame `summaryT`, showing the data types of each column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94611e5a",
      "metadata": {
        "id": "94611e5a"
      },
      "outputs": [],
      "source": [
        "summaryT.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b162b6c3",
      "metadata": {
        "id": "b162b6c3"
      },
      "source": [
        "## Converting Column Data Types in PySpark DataFrame\n",
        "\n",
        "The code snippet converts the data type of columns in the PySpark DataFrame `summaryT` to `DoubleType`:\n",
        "\n",
        "- `from pyspark.sql.types import DoubleType`: Imports the `DoubleType` class from `pyspark.sql.types`. This class represents double precision (64-bit) floating-point numbers.\n",
        "\n",
        "- The loop iterates over each column (`c`) in the DataFrame `summaryT`:\n",
        "  - `if c == 'field': continue`: Skips the 'field' column to avoid attempting to convert it to `DoubleType`.\n",
        "\n",
        "  - `summaryT = summaryT.withColumn(c, summaryT[c].cast(DoubleType()))`: Converts the data type of the column `c` to `DoubleType` using the `cast` method and updates the DataFrame `summaryT` with the new column data type.\n",
        "\n",
        "- `summaryT.printSchema()`: Prints the schema of the PySpark DataFrame `summaryT` after converting column data types, showing the updated data types of each column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "339af78f",
      "metadata": {
        "id": "339af78f"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import DoubleType\n",
        "for c in summaryT.columns:\n",
        "    if c == 'field':\n",
        "        continue\n",
        "summaryT = summaryT.withColumn(c, summaryT[c].cast(DoubleType()))\n",
        "summaryT.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea78e765",
      "metadata": {
        "id": "ea78e765"
      },
      "source": [
        "## Function for Pivoting Summary Statistics in PySpark\n",
        "\n",
        "The code defines a function `pivot_summary` that pivots summary statistics in a PySpark DataFrame:\n",
        "\n",
        "- `from pyspark.sql import DataFrame`: Imports the `DataFrame` class from `pyspark.sql`.\n",
        "\n",
        "- `from pyspark.sql.types import DoubleType`: Imports the `DoubleType` class from `pyspark.sql.types`. This class represents double precision (64-bit) floating-point numbers.\n",
        "\n",
        "- The `pivot_summary` function:\n",
        "  - Accepts a PySpark DataFrame `desc` containing summary statistics.\n",
        "  - Converts the PySpark DataFrame `desc` to a Pandas DataFrame `desc_p` using the `toPandas` method.\n",
        "  - Transposes the Pandas DataFrame `desc_p`, sets the index, and resets the columns.\n",
        "  - Converts the Pandas DataFrame `desc_p` back to a PySpark DataFrame `descT` using the `createDataFrame` method.\n",
        "  - Converts metric columns to `DoubleType` from string in the PySpark DataFrame `descT` using a loop and the `cast` method.\n",
        "  - Returns the pivoted PySpark DataFrame `descT`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6d80ee2",
      "metadata": {
        "id": "f6d80ee2"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.types import DoubleType\n",
        "def pivot_summary(desc):\n",
        "    # convert to pandas dataframe\n",
        "    desc_p = desc.toPandas()\n",
        "    # transpose\n",
        "    desc_p = desc_p.set_index('summary').transpose().reset_index()\n",
        "    desc_p = desc_p.rename(columns={'index':'field'})\n",
        "    desc_p = desc_p.rename_axis(None, axis=1)\n",
        "    # convert to Spark dataframe\n",
        "    descT = spark.createDataFrame(desc_p)\n",
        "    # convert metric columns to double from string\n",
        "    for c in descT.columns:\n",
        "        if c == 'field':\n",
        "            continue\n",
        "        else:\n",
        "            descT = descT.withColumn(c, descT[c].cast(DoubleType()))\n",
        "        return descT\n",
        "match_summaryT = pivot_summary(match_summary)\n",
        "miss_summaryT = pivot_summary(miss_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa5e625",
      "metadata": {
        "id": "caa5e625"
      },
      "source": [
        "## Creating Temporary Views and Executing SQL Query in PySpark\n",
        "\n",
        "The code snippet creates temporary views for the DataFrames `match_summaryT` and `miss_summaryT` and then executes a SQL query to compare summary statistics between the two views:\n",
        "\n",
        "- `match_summaryT.createOrReplaceTempView(\"match_desc\")`: Creates a temporary view named \"match_desc\" from the DataFrame `match_summaryT`.\n",
        "\n",
        "- `miss_summaryT.createOrReplaceTempView(\"miss_desc\")`: Creates a temporary view named \"miss_desc\" from the DataFrame `miss_summaryT`.\n",
        "\n",
        "- The SQL query:\n",
        "  - `SELECT a.field, a.count + b.count total, a.mean - b.mean delta`: Selects the \"field\" column from the \"match_desc\" view, calculates the total count of rows for each field, and computes the difference in mean values between \"match_desc\" and \"miss_desc\" views for each field. The result columns are named \"field\", \"total\", and \"delta\", respectively.\n",
        "  \n",
        "  - `FROM match_desc a INNER JOIN miss_desc b ON a.field = b.field`: Performs an inner join between the \"match_desc\" and \"miss_desc\" views on the \"field\" column to compare summary statistics for each field.\n",
        "\n",
        "  - `WHERE a.field NOT IN (\"id_1\", \"id_2\")`: Filters out fields \"id_1\" and \"id_2\" from the result to focus on other fields.\n",
        "\n",
        "  - `ORDER BY delta DESC, total DESC`: Orders the result by the \"delta\" column in descending order (largest differences first), and then by the \"total\" column in descending order (largest totals first).\n",
        "\n",
        "The SQL query provides insights into how summary statistics differ between matches and misses, excluding the \"id_1\" and \"id_2\" fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f92dd9a2",
      "metadata": {
        "id": "f92dd9a2"
      },
      "outputs": [],
      "source": [
        "match_summaryT.createOrReplaceTempView(\"match_desc\")\n",
        "miss_summaryT.createOrReplaceTempView(\"miss_desc\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT a.field, a.count + b.count total, a.mean - b.mean delta\n",
        "FROM match_desc a INNER JOIN miss_desc b ON a.field = b.field\n",
        "WHERE a.field NOT IN (\"id_1\", \"id_2\")\n",
        "ORDER BY delta DESC, total DESC\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18dd2a55",
      "metadata": {
        "id": "18dd2a55"
      },
      "source": [
        "## Creating Summation Expression for Good Features\n",
        "\n",
        "The code snippet creates a summation expression for the list of good features:\n",
        "\n",
        "- `good_features = [\"cmp_lname_c1\", \"cmp_plz\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]`: Defines a list of good features.\n",
        "\n",
        "- `sum_expression = \" + \".join(good_features)`: Joins the elements of the `good_features` list with the \" + \" separator to create a summation expression. The `join` method concatenates the elements of the list into a single string separated by the specified separator.\n",
        "\n",
        "The `sum_expression` string contains the summation expression for the good features, which can be used in further computations or expressions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc6cb061",
      "metadata": {
        "id": "fc6cb061"
      },
      "outputs": [],
      "source": [
        "good_features = [\"cmp_lname_c1\", \"cmp_plz\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]\n",
        "sum_expression = \" + \".join(good_features)\n",
        "sum_expression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3f8b9e8",
      "metadata": {
        "id": "a3f8b9e8"
      },
      "source": [
        "## Calculating Score for Good Features in PySpark DataFrame\n",
        "\n",
        "The code snippet calculates a score for the good features in the PySpark DataFrame `parsed` and displays the result:\n",
        "\n",
        "- `from pyspark.sql.functions import expr`: Imports the `expr` function from `pyspark.sql.functions`. This function allows you to create a new column in a DataFrame by evaluating a string expression.\n",
        "\n",
        "- `scored = parsed.fillna(0, subset=good_features).withColumn('score', expr(sum_expression)).select('score', 'is_match')`: Calculates the score for the good features in the `parsed` DataFrame. The `fillna` method is used to replace null values with 0 in the good features columns. The `withColumn` method adds a new column named 'score' to the DataFrame, which is calculated using the `sum_expression`. Finally, the `select` method is used to select only the 'score' and 'is_match' columns from the DataFrame.\n",
        "\n",
        "- `scored.show()`: Displays the 'score' and 'is_match' columns of the `scored` DataFrame, showing the calculated score for each row along with the 'is_match' value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "547145b2",
      "metadata": {
        "id": "547145b2"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "scored = parsed.fillna(0, subset=good_features).\\\n",
        "withColumn('score', expr(sum_expression)).\\\n",
        "select('score', 'is_match')\n",
        "scored.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44401678",
      "metadata": {
        "id": "44401678"
      },
      "source": [
        "## Creating CrossTabs in PySpark\n",
        "\n",
        "The code defines a function `crossTabs` that creates cross-tabulations for a given DataFrame `scored` and threshold `t`:\n",
        "\n",
        "- `def crossTabs(scored: DataFrame, t: DoubleType) -> DataFrame:`: Defines a function `crossTabs` that takes a DataFrame `scored` and a threshold `t` as input and returns a DataFrame as output.\n",
        "\n",
        "- `return scored.selectExpr(f\"score >= {t} as above\", \"is_match\").\\`: Selects the 'score' column and creates a new column 'above' that indicates whether the score is above or equal to the threshold `t`. It also selects the 'is_match' column.\n",
        "\n",
        "- `groupBy(\"above\").pivot(\"is_match\", (\"true\", \"false\")).\\`: Groups the DataFrame by the 'above' column and creates a pivot table with 'is_match' as the pivot column. The pivot table shows the count of 'true' and 'false' values for each 'above' value.\n",
        "\n",
        "- `count()`: Calculates the count of occurrences for each combination of 'above' and 'is_match'.\n",
        "\n",
        "The function returns a DataFrame containing the cross-tabulations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29a23bba",
      "metadata": {
        "id": "29a23bba"
      },
      "outputs": [],
      "source": [
        "def crossTabs(scored: DataFrame, t: DoubleType) -> DataFrame:\n",
        "    return scored.selectExpr(f\"score >= {t} as above\", \"is_match\").\\\n",
        "    groupBy(\"above\").pivot(\"is_match\", (\"true\", \"false\")).\\\n",
        "    count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c25c1a87",
      "metadata": {
        "id": "c25c1a87"
      },
      "source": [
        "## Displaying CrossTabs Result in PySpark\n",
        "\n",
        "The code snippet calls the `crossTabs` function with the `scored` DataFrame and a threshold of `4.0`, and then displays the result:\n",
        "\n",
        "- `crossTabs(scored, 4.0)`: Calls the `crossTabs` function with the `scored` DataFrame and a threshold of `4.0` to calculate cross-tabulations.\n",
        "\n",
        "- `show()`: Displays the result of the cross-tabulations, showing the count of 'true' and 'false' values for each 'above' value (indicating whether the score is above or equal to the threshold).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3585b141",
      "metadata": {
        "id": "3585b141"
      },
      "outputs": [],
      "source": [
        "crossTabs(scored, 4.0).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40e0bec3",
      "metadata": {
        "id": "40e0bec3"
      },
      "source": [
        "## Displaying CrossTabs Result in PySpark\n",
        "\n",
        "The code snippet calls the `crossTabs` function with the `scored` DataFrame and a threshold of `2.0`, and then displays the result:\n",
        "\n",
        "- `crossTabs(scored, 2.0)`: Calls the `crossTabs` function with the `scored` DataFrame and a threshold of `2.0` to calculate cross-tabulations.\n",
        "\n",
        "- `show()`: Displays the result of the cross-tabulations, showing the count of 'true' and 'false' values for each 'above' value (indicating whether the score is above or equal to the threshold).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6d94be4",
      "metadata": {
        "id": "c6d94be4"
      },
      "outputs": [],
      "source": [
        "crossTabs(scored, 2.0).show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
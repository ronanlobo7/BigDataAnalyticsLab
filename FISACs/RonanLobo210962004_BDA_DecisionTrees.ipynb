{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Decision Trees\n",
        "\n",
        "In this Jupyter notebook, we will explore the implementation of decision trees using PySpark. Decision trees are a powerful machine learning algorithm for classification and regression tasks, capable of handling both categorical and numerical data.\n"
      ],
      "metadata": {
        "id": "owkUx9sDAoz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up PySpark Environment\n",
        "\n",
        "We begin by importing the necessary libraries for working with PySpark, including `pyspark`, `os`, `SparkSession` from `pyspark.sql`, `DoubleType` from `pyspark.sql.types`, and `col` from `pyspark.sql.functions`. These libraries are essential for initializing a PySpark session, defining data types, and performing dataframe operations.\n"
      ],
      "metadata": {
        "id": "SJSQKKjP_l6M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDiq5K_X-S8w"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import col"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing Spark Session\n",
        "\n",
        "We create a SparkSession named `spark` using the `SparkSession.builder.getOrCreate()` method. This initializes a Spark session if it doesn't exist already or retrieves an existing one. The SparkSession is essential for interacting with Spark functionality and executing Spark jobs.\n"
      ],
      "metadata": {
        "id": "V6GQFELBA7Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "XqC2Gn40AtX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading Data Without Header\n",
        "\n",
        "We use `spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"data/covtype.data\")` to read the dataset `covtype.data` without considering the first row as a header. The `inferSchema` option is set to `True` to automatically infer the schema of the dataset. Subsequently, we print the summary statistics of the dataset using `data_without_header.summary`.\n"
      ],
      "metadata": {
        "id": "e3UVrMIzA-lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_without_header = spark.read.option(\"inferSchema\",True).option(\"header\",False).csv(\"data/covtype.data\")\n",
        "print(data_without_header.summary)"
      ],
      "metadata": {
        "id": "JHZC-juvA9Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Column Names\n",
        "\n",
        "We define a list `colnames` containing the names of columns for our dataset. The list includes features such as elevation, aspect, slope, distances to hydrology, roadways, fire points, hillshade values, as well as wilderness area and soil type indicators. The last column represents the cover type, which is the target variable.\n"
      ],
      "metadata": {
        "id": "NuIUUWffBTlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colnames = [\"Elevation\",\"Aspect\",\"Slope\",\"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\",\\\n",
        "           \"Horizontal_Distance_To_Roadways\",\"Hillshade_9am\",\"Hillshade_noon\",\"Hillshade_3pm\",\\\n",
        "           \"Horizontal_Distance_To_Fire_Points\"] + \\\n",
        "           [f\"Wilderness_Area_{i}\" for i in range(4)] + [f\"Soil_Type_{i}\" for i in range(40)] + [\"Cover_Type\"]"
      ],
      "metadata": {
        "id": "0uc-4yozBFGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Printing Schema of Data\n",
        "\n",
        "We use the `printSchema()` method on the `data_without_header` DataFrame to display the schema of the dataset. This schema outlines the data types and structure of each column, providing insights into the dataset's organization and format.\n"
      ],
      "metadata": {
        "id": "G8Nbp7ZcBaIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_without_header.printSchema()"
      ],
      "metadata": {
        "id": "Y1s4oRIXA9VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting Data and Adjusting Column Types\n",
        "\n",
        "We convert the `data_without_header` DataFrame to a new DataFrame named `data`, using the column names defined earlier (`colnames`). Additionally, we adjust the data type of the \"Cover_Type\" column to `DoubleType()` using the `withColumn()` method and `col()` function from PySpark's `pyspark.sql.functions` module. Finally, we print the summary statistics of the `data` DataFrame using `data.summary`.\n"
      ],
      "metadata": {
        "id": "Yrbgvk4KBoej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_without_header.toDF(*colnames).withColumn(\"Cover_Type\",col(\"Cover_Type\").cast(DoubleType()))\n",
        "print(data.summary)"
      ],
      "metadata": {
        "id": "Cb9lQo0VA9Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting Data into Training and Testing Sets\n",
        "\n",
        "We split the `data` DataFrame into training and testing sets using the `randomSplit()` method. The training set (`train_data`) contains 90% of the data, while the testing set (`test_data`) contains 10%. This ensures that we have separate datasets for training and evaluating our machine learning models.\n"
      ],
      "metadata": {
        "id": "M_rXrcF3Bt_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data,test_data = data.randomSplit([0.9,0.1])"
      ],
      "metadata": {
        "id": "kRJNClahA9P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Feature Vectors\n",
        "\n",
        "We use the `VectorAssembler` from `pyspark.ml.feature` to assemble the feature vectors for training data. First, we define the input columns (`input_cols`) excluding the target variable. Then, we create a `VectorAssembler` object specifying the input and output columns. Next, we transform the training data using the `vector_assembler`, and finally, we display the assembled feature vectors for training data.\n"
      ],
      "metadata": {
        "id": "NP_y5mVyBzOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "input_cols = colnames[:-1]\n",
        "vector_assembler = VectorAssembler(inputCols = input_cols,outputCol = \"featureVector\")\n",
        "assembled_train_data = vector_assembler.transform(train_data)\n",
        "\n",
        "assembled_train_data.select(\"featureVector\").show(truncate=False)"
      ],
      "metadata": {
        "id": "nS_M8ZsCA9NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Decision Tree Classifier\n",
        "\n",
        "We instantiate a `DecisionTreeClassifier` from `pyspark.ml.classification` with specified parameters such as `seed`, `labelCol`, `featuresCol`, and `predictionCol`. Then, we train the classifier on the assembled training data (`assembled_train_data`) using the `fit()` method, resulting in the `model`. Finally, we print the debug string representation of the trained decision tree model using `model.toDebugString`.\n"
      ],
      "metadata": {
        "id": "hW7fNTt8B1QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "classifier = DecisionTreeClassifier(seed=1234,labelCol=\"Cover_Type\",featuresCol=\"featureVector\",predictionCol=\"prediction\")\n",
        "model=classifier.fit(assembled_train_data)\n",
        "print(model.toDebugString)"
      ],
      "metadata": {
        "id": "69_wBBZPA9KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Importance Analysis\n",
        "\n",
        "We import the Pandas library as `pd` and create a DataFrame to analyze the feature importances of the trained decision tree model (`model`). The `featureImportances` attribute of the model is converted to a NumPy array and then to a DataFrame, indexed by the input columns (`input_cols`). We sort the DataFrame by the importance of features in descending order to identify the most influential features.\n"
      ],
      "metadata": {
        "id": "flUNuXaIB20U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(model.featureImportances.toArray(),index=input_cols,columns=['importance']).sort_values(by=\"importance\",ascending=False)"
      ],
      "metadata": {
        "id": "4lgv9070B3hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Model Performance\n",
        "\n",
        "We make predictions on the assembled training data (`assembled_train_data`) using the trained decision tree model (`model`). The predictions include the actual cover type (`Cover_Type`), predicted cover type (`prediction`), and the probability distribution across classes (`probability`). We then use a `MulticlassClassificationEvaluator` to evaluate the model's performance based on accuracy and F1-score metrics. Finally, we print the computed accuracy and F1 score.\n"
      ],
      "metadata": {
        "id": "KSvLE-89B317"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "predictions = model.transform(assembled_train_data)\n",
        "predictions.select(\"Cover_Type\",\"prediction\",\"probability\").show(10,truncate=False)\n",
        "\n",
        "evaluator=MulticlassClassificationEvaluator(labelCol=\"Cover_Type\",predictionCol=\"prediction\")\n",
        "acc = evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
        "f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
        "\n",
        "print('Accuracy: ', acc)\n",
        "print('F1 Score: ', f1)"
      ],
      "metadata": {
        "id": "cMsEit_WB4CZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
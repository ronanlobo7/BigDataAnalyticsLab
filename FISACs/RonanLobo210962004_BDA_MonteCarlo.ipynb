{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Estimating Financial Risk Through Monte Carlo Simulation\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Financial risk management is a critical aspect of modern investment strategies. It involves assessing potential risks in financial markets and implementing strategies to mitigate them. One powerful tool used in risk assessment is Monte Carlo simulation.\n",
        "\n",
        "Monte Carlo simulation is a statistical technique used to model the probability of different outcomes in a process that cannot easily be predicted due to the intervention of random variables. In the context of finance, Monte Carlo simulation can be employed to estimate the potential range of outcomes for various investment portfolios, helping investors and financial analysts make informed decisions.\n",
        "\n",
        "In this project, we will explore how to implement Monte Carlo simulation using PySpark, a powerful framework for big data processing, to estimate financial risk. By leveraging PySpark's distributed computing capabilities, we can efficiently simulate large portfolios and derive meaningful insights into their risk profiles.\n"
      ],
      "metadata": {
        "id": "wS9KJd0Kf4wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing PySpark Libraries\n",
        "\n",
        "We import necessary libraries for working with PySpark, including `pyspark`, `os`, and `sys`. Additionally, we import `SparkContext` and `SparkSession` from `pyspark` to initialize and manage the Spark context and session, respectively.\n"
      ],
      "metadata": {
        "id": "NrQTtwItSCMS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOvUD3cRRuQ6"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import os\n",
        "import sys\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing Spark Session with Custom Configuration\n",
        "\n",
        "We initialize a Spark session named `chapter_8` using `SparkSession.builder`. We configure the driver memory to use 16 GB with `.config(\"spark.driver.memory\", \"16g\")`. This ensures that the driver application has sufficient memory for processing tasks.\n"
      ],
      "metadata": {
        "id": "BvO_dZ_hSDn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_8').getOrCreate()"
      ],
      "metadata": {
        "id": "zN8YKdQ5SDzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Stock Data\n",
        "\n",
        "We load stock data from multiple CSV files (`ABAX.csv`, `AAME.csv`, `AEPI.csv`) located in the directory \"data/stocksA\". We use `spark.read.csv()` method, specifying the list of file paths, enabling header and schema inference with `header='true'` and `inferSchema='true'` parameters, respectively. Finally, we display the first two rows of the combined DataFrame using `stocks.show(2)`.\n"
      ],
      "metadata": {
        "id": "VCeFcFOCSD-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stocks = spark.read.csv([\"data/stocksA/ABAX.csv\",\"data/stocksA/AAME.csv\",\"data/stocksA/AEPI.csv\"], header='true', inferSchema='true')\n",
        "\n",
        "stocks.show(2)"
      ],
      "metadata": {
        "id": "ZQMr6qEJSEJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Symbol Information\n",
        "\n",
        "We extract the symbol information from the file paths of the stock data using PySpark's `functions` module. We add a new column \"Symbol\" to the `stocks` DataFrame, extracting the filename from the file path. Then, we split the filename by \"/\" to remove the directory path and by \".\" to extract the symbol. Finally, we display the first two rows of the modified DataFrame.\n"
      ],
      "metadata": {
        "id": "itTKonYgSEfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as fun\n",
        "\n",
        "stocks = stocks.withColumn(\"Symbol\", fun.input_file_name()).\\\n",
        "  withColumn(\"Symbol\", fun.element_at(fun.split(\"Symbol\", \"/\"), -1)).\\\n",
        "  withColumn(\"Symbol\", fun.element_at(fun.split(\"Symbol\", \"\\.\"), 1))\n",
        "\n",
        "stocks.show(2)"
      ],
      "metadata": {
        "id": "S55b32LUSErX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Extracting Symbol Information for Factors Data\n",
        "\n",
        "We load factor data from multiple CSV files (`ABAX.csv`, `AAME.csv`, `AEPI.csv`) located in the directory \"data/stocksA\". We use `spark.read.csv()` method, specifying the list of file paths, enabling header and schema inference with `header='true'` and `inferSchema='true'` parameters, respectively. Then, we add a new column \"Symbol\" to the `factors` DataFrame, extracting the filename from the file path. We split the filename by \"/\" to remove the directory path and by \".\" to extract the symbol.\n"
      ],
      "metadata": {
        "id": "CP_OLVxdSE2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "factors = spark.read.csv([\"data/stocksA/ABAX.csv\",\"data/stocksA/AAME.csv\",\"data/stocksA/AEPI.csv\"], header='true', inferSchema='true')\n",
        "\n",
        "factors = factors.withColumn(\"Symbol\", fun.input_file_name()).\\\n",
        "  withColumn(\"Symbol\", fun.element_at(fun.split(\"Symbol\", \"/\"), -1)).\\\n",
        "  withColumn(\"Symbol\", fun.element_at(fun.split(\"Symbol\", \"\\.\"), 1))"
      ],
      "metadata": {
        "id": "3CIvozeESFCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtering Stocks Data\n",
        "\n",
        "We filter the `stocks` DataFrame based on the count of occurrences of each symbol. Using PySpark's `Window` function, we partition the data by the \"Symbol\" column and calculate the count of occurrences for each symbol. We then filter the DataFrame to include only symbols with counts greater than 260 times 5 plus 10.\n"
      ],
      "metadata": {
        "id": "s5Ea-zoQSFOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window\n",
        "\n",
        "stocks = stocks.withColumn('count', fun.count('Symbol').\\\n",
        "  over(Window.partitionBy('Symbol'))).\\\n",
        "  filter(fun.col('count') > 260*5 + 10)"
      ],
      "metadata": {
        "id": "-CKfryD9SFcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuring Legacy Time Parser Policy\n",
        "\n",
        "We configure the Spark SQL session to use the legacy time parser policy by executing the SQL command `spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")`. This ensures compatibility with legacy time parsing behavior in Spark SQL.\n"
      ],
      "metadata": {
        "id": "ch42uq0ISFoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")"
      ],
      "metadata": {
        "id": "XI8FJH3zSFz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing Date Column\n",
        "\n",
        "We parse the \"Date\" column in the `stocks` DataFrame to convert it into a date type. We first convert the column values to timestamps using `to_timestamp()`, specifying the format 'dd-MMM-yy'. Then, we convert the timestamps to dates using `to_date()`. Finally, we display the schema of the modified DataFrame using `stocks.printSchema()`.\n"
      ],
      "metadata": {
        "id": "sa1oG1qSTMT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stocks = stocks.withColumn(\n",
        "    'Date',\n",
        "    fun.to_date(fun.to_timestamp(fun.col('Date'), 'dd-MMM-yy'))\n",
        ")\n",
        "\n",
        "stocks.printSchema()\n"
      ],
      "metadata": {
        "id": "FUcfj2wzTMh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtering Date Range\n",
        "\n",
        "We filter the `stocks` DataFrame to include data within the date range from October 23, 2009, to October 23, 2014. We apply two filter conditions using PySpark's `filter()` function: one to include dates greater than or equal to October 23, 2009, and another to include dates less than or equal to October 23, 2014.\n"
      ],
      "metadata": {
        "id": "ZieTwWSVTMtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "stocks = stocks.filter(fun.col('Date') >= datetime(2009, 10, 23)).\\\n",
        "  filter(fun.col('Date') <= datetime(2014, 10, 23))"
      ],
      "metadata": {
        "id": "VT4yTYkQTM5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing Date Column for Factors Data and Filtering Date Range\n",
        "\n",
        "We parse the \"Date\" column in the `factors` DataFrame to convert it into a date type, similar to the procedure done for the `stocks` DataFrame. Then, we filter the `factors` DataFrame to include data within the date range from October 23, 2009, to October 23, 2014, similar to the procedure applied for the `stocks` DataFrame.\n"
      ],
      "metadata": {
        "id": "jbMgirEGTNFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "factors = factors.withColumn(\n",
        "    'Date',\n",
        "    fun.to_date(fun.to_timestamp(fun.col('Date'), 'dd-MMM-yy'))\n",
        ")\n",
        "\n",
        "factors = factors.filter(fun.col('Date') >= datetime(2009, 10, 23)).\\\n",
        "  filter(fun.col('Date') <= datetime(2014, 10, 23))"
      ],
      "metadata": {
        "id": "JRuZdzXWTNR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting Spark DataFrames to Pandas DataFrames\n",
        "\n",
        "We convert the `stocks` and `factors` Spark DataFrames to Pandas DataFrames using the `toPandas()` method. This allows for easier manipulation and analysis of the data using Pandas. We display the first 5 rows of the `factors_pd_df` DataFrame to inspect the converted data.\n"
      ],
      "metadata": {
        "id": "w7eMgEmvTNfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stocks_pd_df = stocks.toPandas()\n",
        "factors_pd_df = factors.toPandas()\n",
        "\n",
        "factors_pd_df.head(5)"
      ],
      "metadata": {
        "id": "hQlIWWZYTNsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Rolling Returns\n",
        "\n",
        "We define a function `my_fun` to calculate the rolling returns for each stock and factor. We group the data by the \"Symbol\" column and calculate the rolling returns for the \"Close\" column using a rolling window of `n_steps`. The calculated returns are then stored in `stock_returns` and `factors_returns` DataFrames. Finally, we reset the index and sort the dataframes to ensure consistency in the ordering of rows.\n"
      ],
      "metadata": {
        "id": "eU9LhVTRT6mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 10\n",
        "\n",
        "def my_fun(x):\n",
        "  return ((x.iloc[-1] - x.iloc[0]) / x.iloc[0])\n",
        "\n",
        "stock_returns = stocks_pd_df.groupby('Symbol').Close.\\\n",
        "  rolling(window=n_steps).apply(my_fun)\n",
        "\n",
        "factors_returns = factors_pd_df.groupby('Symbol').Close.\\\n",
        "  rolling(window=n_steps).apply(my_fun)\n",
        "\n",
        "stock_returns = stock_returns.reset_index().\\\n",
        "  sort_values('level_1').\\\n",
        "  reset_index()\n",
        "\n",
        "factors_returns = factors_returns.reset_index().\\\n",
        "  sort_values('level_1').\\\n",
        "  reset_index()"
      ],
      "metadata": {
        "id": "eEQ-ItNeT6x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating DataFrames with Returns\n",
        "\n",
        "We create new DataFrames `stocks_pd_df_with_returns` and `factors_pd_df_with_returns` by assigning calculated returns to the original DataFrames `stocks_pd_df` and `factors_pd_df`, respectively. For `stocks_pd_df_with_returns`, we assign the calculated stock returns from `stock_returns['Close']` to a new column named \"stock_returns\". For `factors_pd_df_with_returns`, we assign the calculated factor returns and squared factor returns from `factors_returns['Close']` and `factors_returns['Close']**2` respectively, to new columns named \"factors_returns\" and \"factors_returns_squared\".\n",
        "\n",
        "We then pivot the `factors_pd_df_with_returns` DataFrame to rearrange the data by dates as rows and symbols as columns. The column names are adjusted accordingly. Finally, we reset the index of the DataFrame for consistency and display the first row.\n"
      ],
      "metadata": {
        "id": "ByR2lonGT6-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stocks_pd_df_with_returns = stocks_pd_df.\\\n",
        "  assign(stock_returns=stock_returns['Close'])\n",
        "\n",
        "factors_pd_df_with_returns = factors_pd_df.\\\n",
        "  assign(factors_returns=factors_returns['Close'],factors_returns_squared=factors_returns['Close']**2)\n",
        "\n",
        "factors_pd_df_with_returns = factors_pd_df_with_returns.\\\n",
        "  pivot(index='Date', columns='Symbol', values=['factors_returns', 'factors_returns_squared'])\n",
        "\n",
        "factors_pd_df_with_returns.columns = factors_pd_df_with_returns.\\\n",
        "  columns.to_series().str.join('_').reset_index()[0]\n",
        "\n",
        "factors_pd_df_with_returns = factors_pd_df_with_returns.reset_index()\n",
        "\n",
        "print(factors_pd_df_with_returns.head(1))"
      ],
      "metadata": {
        "id": "Yz5J-ZUyT7Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Displaying Columns of DataFrame\n",
        "\n",
        "We print the columns of the `factors_pd_df_with_returns` DataFrame to inspect the column names after the data transformation and manipulation.\n"
      ],
      "metadata": {
        "id": "C7kLAADMT7XC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(factors_pd_df_with_returns.columns)"
      ],
      "metadata": {
        "id": "mUvJgMVlT7jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing Linear Regression Analysis\n",
        "\n",
        "We merge the `stocks_pd_df_with_returns` and `factors_pd_df_with_returns` DataFrames into `stocks_factors_combined_df` based on the common column \"Date\". We then select the feature columns from the combined DataFrame.\n",
        "\n",
        "We drop rows with missing values in both the feature columns and the target column \"stock_returns\". Then, we define a function `find_ols_coef()` to perform Ordinary Least Squares (OLS) regression for each stock. The function calculates the coefficients for the feature columns.\n",
        "\n",
        "We apply the `find_ols_coef()` function to each group of stocks and store the coefficients in `coefs_per_stock` DataFrame. Finally, we reset the index and rename the columns for better readability.\n"
      ],
      "metadata": {
        "id": "iLL9VaaWT7wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "stocks_factors_combined_df = pd.merge(stocks_pd_df_with_returns,\n",
        "  factors_pd_df_with_returns, how=\"left\", on=\"Date\")\n",
        "\n",
        "feature_columns = list(stocks_factors_combined_df.columns[-6:])\n",
        "\n",
        "with pd.option_context('mode.use_inf_as_na', True):\n",
        "  stocks_factors_combined_df = stocks_factors_combined_df.\\\n",
        "    dropna(subset=feature_columns + ['stock_returns'])\n",
        "\n",
        "def find_ols_coef(df):\n",
        "  y = df[['stock_returns']].values\n",
        "  X = df[feature_columns]\n",
        "  regr = LinearRegression()\n",
        "  regr_output = regr.fit(X, y)\n",
        "  return list(df[['Symbol']].values[0]) + list(regr_output.coef_[0])\n",
        "\n",
        "coefs_per_stock = stocks_factors_combined_df.groupby('Symbol').apply(find_ols_coef)\n",
        "\n",
        "coefs_per_stock = pd.DataFrame(coefs_per_stock).reset_index()\n",
        "coefs_per_stock.columns = ['symbol', 'factor_coef_list']\n",
        "\n",
        "coefs_per_stock = pd.DataFrame(\n",
        "  coefs_per_stock.factor_coef_list.tolist(),\n",
        "  index=coefs_per_stock.index,\n",
        "  columns = ['Symbol'] + feature_columns\n",
        ")\n"
      ],
      "metadata": {
        "id": "6AgUPrx2T79I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting Kernel Density Estimation (KDE) for Factor Returns\n",
        "\n",
        "We select samples of factor returns data for a specific symbol (the first unique symbol in this case) from the `factors_returns` DataFrame. We then plot the Kernel Density Estimation (KDE) of the factor returns using `plot.kde()` function from Pandas. This visualization provides insights into the distribution of factor returns for the selected symbol.\n"
      ],
      "metadata": {
        "id": "y_d7xf9BT8LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = factors_returns.loc[factors_returns.Symbol == factors_returns.Symbol.unique()[0]]['Close']\n",
        "\n",
        "samples.plot.kde()"
      ],
      "metadata": {
        "id": "YKAWANXMT8XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Correlation Matrix for Factor Returns\n",
        "\n",
        "We extract samples of factor returns data for the first three unique symbols from the `factors_returns` DataFrame (`f_1`, `f_2`, `f_3`). Then, we print the sizes of the samples to ensure consistency.\n",
        "\n",
        "We create a DataFrame containing these samples with 1039 data points for each factor, and then calculate the correlation matrix using the `corr()` method. This matrix quantifies the linear relationship between pairs of factor returns, providing insights into their correlation patterns.\n"
      ],
      "metadata": {
        "id": "vYtK_kudT8ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_1 = factors_returns.loc[factors_returns.Symbol == factors_returns.Symbol.unique()[0]]['Close']\n",
        "f_2 = factors_returns.loc[factors_returns.Symbol == factors_returns.Symbol.unique()[1]]['Close']\n",
        "f_3 = factors_returns.loc[factors_returns.Symbol == factors_returns.Symbol.unique()[2]]['Close']\n",
        "\n",
        "print(f_1.size, len(f_2), f_3.size)\n",
        "\n",
        "pd.DataFrame({\n",
        "    'f1': list(f_1)[1:1040],\n",
        "    'f2': list(f_2)[1:1040],\n",
        "    'f3': list(f_3)\n",
        "}).corr()"
      ],
      "metadata": {
        "id": "l5clrCxmT8uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Covariance Matrix and Mean for Factor Returns\n",
        "\n",
        "We create a DataFrame containing samples of factor returns data for the first three unique symbols from the `factors_returns` DataFrame (`f_1`, `f_2`, `f_3`). Then, we calculate the covariance matrix using the `cov()` method and convert it to a NumPy array.\n",
        "\n",
        "Additionally, we calculate the mean of factor returns for each factor using the `mean()` method. These calculations provide insights into the variability and central tendency of factor returns.\n"
      ],
      "metadata": {
        "id": "CV-KWLGLVyf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "factors_returns_cov = pd.DataFrame({\n",
        "    'f1': list(f_1)[1:1040],\n",
        "    'f2': list(f_2)[1:1040],\n",
        "    'f3': list(f_3)\n",
        "}).cov().to_numpy()\n",
        "\n",
        "factors_returns_mean = pd.DataFrame({\n",
        "    'f1': list(f_1)[1:1040],\n",
        "    'f2': list(f_2)[1:1040],\n",
        "    'f3': list(f_3)\n",
        "}).mean()"
      ],
      "metadata": {
        "id": "6VwQTUtZVyrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Multivariate Normal Distribution\n",
        "\n",
        "We generate samples from a multivariate normal distribution using the mean vector `factors_returns_mean` and covariance matrix `factors_returns_cov`. This distribution represents the joint distribution of factor returns for the first three factors. The `multivariate_normal()` function from NumPy's random module is used for this purpose.\n"
      ],
      "metadata": {
        "id": "qWChhsiIVy3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random import multivariate_normal\n",
        "\n",
        "multivariate_normal(factors_returns_mean, factors_returns_cov)"
      ],
      "metadata": {
        "id": "H24D2chNVzEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Broadcasting Variables\n",
        "\n",
        "We broadcast the following variables to all Spark workers for efficient distributed computation:\n",
        "\n",
        "1. `b_coefs_per_stock`: Broadcasts the coefficients per stock DataFrame (`coefs_per_stock`), containing the coefficients calculated from the linear regression analysis.\n",
        "2. `b_feature_columns`: Broadcasts the feature columns list (`feature_columns`), containing the names of the features used in the linear regression analysis.\n",
        "3. `b_factors_returns_mean`: Broadcasts the mean vector of factor returns (`factors_returns_mean`), representing the average returns for each factor.\n",
        "4. `b_factors_returns_cov`: Broadcasts the covariance matrix of factor returns (`factors_returns_cov`), representing the variability and covariance among factor returns.\n"
      ],
      "metadata": {
        "id": "yh1DAhylVzQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b_coefs_per_stock = spark.sparkContext.broadcast(coefs_per_stock)\n",
        "b_feature_columns = spark.sparkContext.broadcast(feature_columns)\n",
        "b_factors_returns_mean = spark.sparkContext.broadcast(factors_returns_mean)\n",
        "b_factors_returns_cov = spark.sparkContext.broadcast(factors_returns_cov)"
      ],
      "metadata": {
        "id": "ANsvHz_GVzcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Seeds DataFrame\n",
        "\n",
        "We generate a DataFrame `seedsDF` containing seeds for random number generation. The number of seeds is determined by the `parallelism` variable, which specifies the level of parallelism. Each seed is generated based on the `base_seed` and incremented by 1 for each parallel execution.\n",
        "\n",
        "The DataFrame is created with an integer type column using `IntegerType()`. We then repartition the DataFrame into `parallelism` partitions to ensure parallel execution across Spark workers.\n"
      ],
      "metadata": {
        "id": "YJ5s05MqVzoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "parallelism = 1000\n",
        "num_trials = 1000000\n",
        "base_seed = 1496\n",
        "\n",
        "seeds = [b for b in range(base_seed, base_seed + parallelism)]\n",
        "\n",
        "seedsDF = spark.createDataFrame(seeds, IntegerType())\n",
        "seedsDF = seedsDF.repartition(parallelism)"
      ],
      "metadata": {
        "id": "ad_hD0HsV0H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining UDF for Calculating Trial Returns\n",
        "\n",
        "We define a Python function `calculate_trial_return(x)` to calculate trial returns for a given seed `x`. Inside the function, we iterate over a range of `num_trials/parallelism` to perform multiple trials in parallel.\n",
        "\n",
        "For each trial, we generate a random integer using Python's `random.randint()` function and set the random seed using `seed(x)` to ensure reproducibility. We then generate random factors from a multivariate normal distribution using the mean and covariance matrix of factor returns stored in the broadcast variables `b_factors_returns_mean` and `b_factors_returns_cov`.\n",
        "\n",
        "Next, we retrieve the coefficients per stock DataFrame from the broadcast variable `b_coefs_per_stock`. We calculate returns per stock by multiplying the coefficients with the random factors and squared random factors. Finally, we sum the returns across all stocks and append the trial return to a list.\n",
        "\n",
        "The function returns a list containing trial returns for each iteration.\n",
        "\n",
        "We define a User Defined Function (UDF) `udf_return` using `udf()` function from PySpark's `functions` module to apply the `calculate_trial_return()` function to each row of the DataFrame.\n"
      ],
      "metadata": {
        "id": "fhN_Wl8AV0Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from numpy.random import seed\n",
        "from pyspark.sql.types import LongType, ArrayType, DoubleType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "def calculate_trial_return(x):\n",
        "  trial_return_list = []\n",
        "\n",
        "  for i in range(int(num_trials/parallelism)):\n",
        "    random_int = random.randint(0, num_trials*num_trials)\n",
        "    seed(x)\n",
        "    random_factors = multivariate_normal(b_factors_returns_mean.value, b_factors_returns_cov.value)\n",
        "    coefs_per_stock_df = b_coefs_per_stock.value\n",
        "    returns_per_stock = (coefs_per_stock_df[b_feature_columns.value] *\n",
        "      (list(random_factors) + list(random_factors**2)))\n",
        "    trial_return_list.append(float(returns_per_stock.sum(axis=1).sum(), b_coefs_per_stock.value.size))\n",
        "\n",
        "  return trial_return_list\n",
        "\n",
        "udf_return = udf(calculate_trial_return, ArrayType(DoubleType()))"
      ],
      "metadata": {
        "id": "7KGbLkrTV0iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing Trials for Calculating Returns\n",
        "\n",
        "We apply the `udf_return` UDF to each row of the `seedsDF` DataFrame to calculate trial returns for each seed. We create a new column \"trial_return\" containing the trial returns for each seed.\n",
        "\n",
        "Next, we use the `explode()` function from PySpark's `functions` module to explode the \"trial_return\" array column into separate rows. This creates a new DataFrame `trials` where each row corresponds to a single trial return.\n",
        "\n",
        "Finally, we cache the `trials` DataFrame in memory for faster access during subsequent computations.\n"
      ],
      "metadata": {
        "id": "J-X6snRVXa2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, explode\n",
        "\n",
        "trials = seedsDF.withColumn(\"trial_return\", udf_return(col(\"value\")))\n",
        "trials = trials.select('value', explode('trial_return').alias('trial_return'))\n",
        "\n",
        "trials.cache()"
      ],
      "metadata": {
        "id": "-_SwiTBNXbCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing Trial Returns\n",
        "\n",
        "We first approximate the 5th percentile of trial returns from the `trials` DataFrame using the `approxQuantile()` function. This provides an estimate of the 5th percentile of trial returns with minimal error.\n",
        "\n",
        "Next, we order the `trials` DataFrame by trial returns in ascending order, select the first 5% of rows representing the lowest returns (limiting to `trials.count()/20` rows), and calculate the average trial return for this subset. This provides insight into the average performance of the worst 5% of trials.\n"
      ],
      "metadata": {
        "id": "hWHHt-nVXbO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trials.approxQuantile('trial_return', [0.05], 0.0)\n",
        "\n",
        "trials.orderBy(col('trial_return').asc()).\\\n",
        "  limit(int(trials.count()/20)).\\\n",
        "  agg(fun.avg(col(\"trial_return\"))).show()"
      ],
      "metadata": {
        "id": "Z5nNFUV2XblB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing Trial Returns\n",
        "\n",
        "We convert the `trials` DataFrame to a Pandas DataFrame named `mytrials`. Then, we plot the trial returns using the `plot.line()` function from Pandas. This visualization provides a line plot of trial returns over time or trial index, allowing for analysis of the distribution and behavior of trial returns.\n"
      ],
      "metadata": {
        "id": "Xi9wtuiYXb0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "\n",
        "mytrials=trials.toPandas()\n",
        "mytrials.plot.line()"
      ],
      "metadata": {
        "id": "T0Dht3TOXcDo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
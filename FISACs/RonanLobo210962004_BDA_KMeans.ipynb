{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QelBfMWbe8aO"
      },
      "source": [
        "# KMEANS CLUSTERING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukt6jr-7e8aS"
      },
      "source": [
        "## Importing necessary libraries\n",
        "- `from pyspark.sql import SparkSession`: Imports the `SparkSession` class from `pyspark.sql`. `SparkSession` is the entry point to programming Spark with the Dataset and DataFrame API.\n",
        "\n",
        "- `spark = SparkSession.builder.getOrCreate()`: Creates a SparkSession `spark` if it doesn't already exist, or gets the existing one. The `builder` method is used to create a `SparkSession`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGYYGoAoe8aT",
        "outputId": "ea591600-a2c2-430a-dbab-5a98ab324258"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
            "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_5').getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edF3t5n3e8aW"
      },
      "source": [
        "## Reading a CSV File Without Header in PySpark\n",
        "\n",
        "The code snippet reads a CSV file without a header into a PySpark DataFrame and prints a summary of the DataFrame:\n",
        "\n",
        "- `data_without_header = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"kddcup.data_10_percent_corrected\")\n",
        "`: Reads a CSV file without a header into a PySpark DataFrame `data_without_header`. The options `inferSchema=True` and `header=False` are used to infer the schema from the data and indicate that the file does not have a header row.\n",
        "\n",
        "- `print(data_without_header.summary)`: Prints a summary of the DataFrame `data_without_header`. The `summary` method provides summary statistics for each numerical column in the DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoTxaCTje8aW"
      },
      "outputs": [],
      "source": [
        "data_without_header = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"kddcup.data_10_percent_corrected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDrYeSMGe8aX"
      },
      "source": [
        "## Defining column names for the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihFyN4Lwe8aX"
      },
      "outputs": [],
      "source": [
        "column_names = [\n",
        "    \"duration\", \"protocol_type\", \"service\", \"flag\",\n",
        "    \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
        "    \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
        "    \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
        "    \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
        "    \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
        "    \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
        "    \"dst_host_count\", \"dst_host_srv_count\",\n",
        "    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
        "    \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
        "    \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
        "    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
        "    \"label\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrjGoVdFe8aX"
      },
      "source": [
        "## Converting the DataFrame without header to a DataFrame with column names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWJ0QGtIe8aY"
      },
      "outputs": [],
      "source": [
        "data = data_without_header.toDF(*column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQoVVD7ge8aY"
      },
      "source": [
        "## Import col\n",
        "\n",
        "- `col` function from `pyspark.sql.functions`: Used to refer to a column in a DataFrame. It allows you to access and manipulate columns when working with PySpark DataFrames.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy-V6C98e8aY"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTb907Pde8aY"
      },
      "source": [
        "## Task Description: Selecting, Grouping, Counting, Ordering, and Showing Top 25 Results\n",
        "\n",
        "1. **Selecting \"label\" Column**: Select the \"label\" column from the DataFrame.\n",
        "\n",
        "2. **Grouping by \"label\" Column**: Group the DataFrame by the \"label\" column.\n",
        "\n",
        "3. **Counting Occurrences**: Count the occurrences of each unique label.\n",
        "\n",
        "4. **Ordering by Count**: Order the result by the count of occurrences in descending order.\n",
        "\n",
        "5. **Showing Top 25 Results**: Display the top 25 results after ordering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyuY7j6Ke8aZ",
        "outputId": "85f529c0-fa6a-4f5b-d1ee-c3541190f157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+------+\n",
            "|           label| count|\n",
            "+----------------+------+\n",
            "|          smurf.|280790|\n",
            "|        neptune.|107201|\n",
            "|         normal.| 97278|\n",
            "|           back.|  2203|\n",
            "|          satan.|  1589|\n",
            "|        ipsweep.|  1247|\n",
            "|      portsweep.|  1040|\n",
            "|    warezclient.|  1020|\n",
            "|       teardrop.|   979|\n",
            "|            pod.|   264|\n",
            "|           nmap.|   231|\n",
            "|   guess_passwd.|    53|\n",
            "|buffer_overflow.|    30|\n",
            "|           land.|    21|\n",
            "|    warezmaster.|    20|\n",
            "|           imap.|    12|\n",
            "|        rootkit.|    10|\n",
            "|     loadmodule.|     9|\n",
            "|      ftp_write.|     8|\n",
            "|       multihop.|     7|\n",
            "|            phf.|     4|\n",
            "|           perl.|     3|\n",
            "|            spy.|     2|\n",
            "+----------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.select(\"label\").groupBy(\"label\").count().orderBy(col(\"count\").desc()).show(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMSs8ot1e8aZ"
      },
      "source": [
        "## Task Description: Creating a Pipeline for KMeans Clustering\n",
        "\n",
        "1. **Importing Required Classes**: Import the `VectorAssembler` class from `pyspark.ml.feature`, the `KMeans` class from `pyspark.ml.clustering`, and the `Pipeline` class from `pyspark.ml`.\n",
        "\n",
        "2. **Creating VectorAssembler**: Create a `VectorAssembler` instance to assemble feature columns into a single feature vector column.\n",
        "\n",
        "3. **Creating KMeans Model**: Create a `KMeans` instance to define the KMeans clustering model, specifying parameters such as the number of clusters.\n",
        "\n",
        "4. **Creating Pipeline**: Create a `Pipeline` instance and set its stages to include the `VectorAssembler` and `KMeans` stages.\n",
        "\n",
        "5. **Overall Purpose**: The pipeline is used to transform the input DataFrame by assembling the features into a vector and then applying KMeans clustering to the feature vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV79umiCe8aa"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joCtBhaze8aa"
      },
      "source": [
        "#### Dropping non-numeric columns and caching the DataFrame\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiVTAGN5e8aa"
      },
      "outputs": [],
      "source": [
        "numeric_only = data.drop(\"protocol_type\", \"service\", \"flag\").cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH_OPu0Ve8aa"
      },
      "source": [
        "#### Creating a VectorAssembler to assemble the feature columns into a single feature vector column\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChHv9NOke8ab"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler().setInputCols(numeric_only.columns[:-1]).setOutputCol(\"featureVector\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ1EDkwVe8ab"
      },
      "source": [
        "#### Creating a KMeans model with the prediction column set to \"cluster\" and features column set to \"featureVector\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNRW9R9Ae8ab"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans().setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaeT-uQ1e8ab"
      },
      "source": [
        "#### Creating a pipeline with the VectorAssembler and KMeans model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F37Gj6Lpe8ac"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline().setStages([assembler, kmeans])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3G4ePe0e8ac"
      },
      "source": [
        "#### Fitting the pipeline to the numeric_only DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2_Sajd8e8ac"
      },
      "outputs": [],
      "source": [
        "pipeline_model = pipeline.fit(numeric_only)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_bBDaDte8ac"
      },
      "source": [
        "#### Extracting the KMeans model from the pipeline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i449BLMGe8ac"
      },
      "outputs": [],
      "source": [
        "kmeans_model = pipeline_model.stages[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9QGrvHFe8ad"
      },
      "source": [
        "#### Printing the cluster centers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8W_qce5e8ad",
        "outputId": "18a879be-fc7d-43fc-d87f-4014b2a63374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[array([4.79793956e+01, 1.62207883e+03, 8.68534183e+02, 4.45326100e-05,\n",
            "       6.43293794e-03, 1.41694668e-05, 3.45168212e-02, 1.51815716e-04,\n",
            "       1.48247035e-01, 1.02121372e-02, 1.11331525e-04, 3.64357718e-05,\n",
            "       1.13517671e-02, 1.08295211e-03, 1.09307315e-04, 1.00805635e-03,\n",
            "       0.00000000e+00, 0.00000000e+00, 1.38658354e-03, 3.32286248e+02,\n",
            "       2.92907143e+02, 1.76685418e-01, 1.76607809e-01, 5.74330999e-02,\n",
            "       5.77183920e-02, 7.91548844e-01, 2.09816404e-02, 2.89968625e-02,\n",
            "       2.32470732e+02, 1.88666046e+02, 7.53781203e-01, 3.09056111e-02,\n",
            "       6.01935529e-01, 6.68351484e-03, 1.76753957e-01, 1.76441622e-01,\n",
            "       5.81176268e-02, 5.74111170e-02]),\n",
            " array([2.0000000e+00, 6.9337564e+08, 0.0000000e+00, 0.0000000e+00,\n",
            "       0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
            "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
            "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
            "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 5.7000000e+01,\n",
            "       3.0000000e+00, 7.9000000e-01, 6.7000000e-01, 2.1000000e-01,\n",
            "       3.3000000e-01, 5.0000000e-02, 3.9000000e-01, 0.0000000e+00,\n",
            "       2.5500000e+02, 3.0000000e+00, 1.0000000e-02, 9.0000000e-02,\n",
            "       2.2000000e-01, 0.0000000e+00, 1.8000000e-01, 6.7000000e-01,\n",
            "       5.0000000e-02, 3.3000000e-01])]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "pprint(kmeans_model.clusterCenters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDwSmpQme8ad"
      },
      "source": [
        "#### Transforming the numeric_only DataFrame using the pipeline model to add a \"cluster\" column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9GpAcSje8ae"
      },
      "outputs": [],
      "source": [
        "with_cluster = pipeline_model.transform(numeric_only)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuX6BXQre8ae"
      },
      "source": [
        "#### Selecting the \"cluster\" and \"label\" columns from the transformed DataFrame\n",
        "grouping by \"cluster\" and \"label\", counting the occurrences of each combination,\n",
        "and ordering the result by \"cluster\" and count in descending order. Also showing top 25 columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "gt15wuhIe8ae",
        "outputId": "8caeaef4-4bea-4711-8217-5b60ccfa1441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------------+------+\n",
            "|cluster|           label| count|\n",
            "+-------+----------------+------+\n",
            "|      0|          smurf.|280790|\n",
            "|      0|        neptune.|107201|\n",
            "|      0|         normal.| 97278|\n",
            "|      0|           back.|  2203|\n",
            "|      0|          satan.|  1589|\n",
            "|      0|        ipsweep.|  1247|\n",
            "|      0|      portsweep.|  1039|\n",
            "|      0|    warezclient.|  1020|\n",
            "|      0|       teardrop.|   979|\n",
            "|      0|            pod.|   264|\n",
            "|      0|           nmap.|   231|\n",
            "|      0|   guess_passwd.|    53|\n",
            "|      0|buffer_overflow.|    30|\n",
            "|      0|           land.|    21|\n",
            "|      0|    warezmaster.|    20|\n",
            "|      0|           imap.|    12|\n",
            "|      0|        rootkit.|    10|\n",
            "|      0|     loadmodule.|     9|\n",
            "|      0|      ftp_write.|     8|\n",
            "|      0|       multihop.|     7|\n",
            "|      0|            phf.|     4|\n",
            "|      0|           perl.|     3|\n",
            "|      0|            spy.|     2|\n",
            "|      1|      portsweep.|     1|\n",
            "+-------+----------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with_cluster.select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(col(\"cluster\"), col(\"count\").desc()).show(25)  # Showing the top 25 results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xnaej5oe8af"
      },
      "source": [
        "### Choosing k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1cykNbNe8af"
      },
      "source": [
        "#### Import Summary\n",
        "\n",
        "- `DataFrame` class from `pyspark.sql`: Used to represent a distributed collection of data organized into named columns.\n",
        "- `randint` function from `random`: Used to generate a random integer between specified integers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "catEmE-6e8af"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from random import randint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W6II5Uye8ag"
      },
      "source": [
        "\n",
        "## **Function Summary: `clustering_score`\n",
        "\n",
        "1. **Input Data Preparation**: It takes a DataFrame `input_data` as input and drops non-numeric columns (`\"protocol_type\"`, `\"service\"`, `\"flag\"`) to create a new DataFrame `input_numeric_only`.\n",
        "\n",
        "2. **Feature Vector Assembly**: It uses `VectorAssembler` to assemble the feature columns of `input_numeric_only` (excluding the last column, which is assumed to be the label column) into a single feature vector column called `\"featureVector\"`.\n",
        "\n",
        "3. **KMeans Model Creation**: It creates a KMeans model with a randomly generated seed and a specified number of clusters `k`, using the feature vector column `\"featureVector\"` for clustering and setting the prediction column name to `\"cluster\"`.\n",
        "\n",
        "4. **Pipeline Creation and Fitting**: It creates a pipeline with the `VectorAssembler` and KMeans model, and fits the pipeline to the `input_numeric_only` DataFrame to train the KMeans model.\n",
        "\n",
        "5. **Training Cost Extraction**: It extracts the trained KMeans model from the pipeline model and retrieves the training cost (sum of squared distances of points to their nearest cluster center) from the model's summary.\n",
        "\n",
        "6. **Return Value**: It returns the training cost of the KMeans model.\n",
        "\n",
        "7. **Iterating Over K Values**: It iterates over a range of `k` values (20, 40, 60, 80) and prints the training cost for each `k`.\n",
        "\n",
        "Overall, the function `clustering_score` is used to train KMeans clustering models with different numbers of clusters and evaluate their training costs, helping to determine the optimal number of clusters for the given dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqN5ih93e8ag"
      },
      "outputs": [],
      "source": [
        "def clustering_score(input_data, k):\n",
        "    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
        "    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
        "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
        "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
        "    pipeline_model = pipeline.fit(input_numeric_only)\n",
        "    kmeans_model = pipeline_model.stages[-1]\n",
        "    training_cost = kmeans_model.summary.trainingCost\n",
        "    return training_cost\n",
        "for k in list(range(20,100, 20)):\n",
        "    print(clustering_score(numeric_only, k))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7VsK7uWe8ag"
      },
      "source": [
        "## Feature normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pfG3qP3e8ah"
      },
      "source": [
        "#### `StandardScaler`\n",
        "\n",
        "The `StandardScaler` class in PySpark's MLlib is used for standardizing features by removing the mean and scaling to unit variance. It is commonly used in machine learning pipelines to scale numerical features before model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76_EanXme8ah"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StandardScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1w0_TYWe8ai"
      },
      "source": [
        "## Function Summary: `clustering_score_2`\n",
        "\n",
        "1. **Input Data Preparation**: It takes a DataFrame `input_data` as input and drops non-numeric columns (`\"protocol_type\"`, `\"service\"`, `\"flag\"`) to create a new DataFrame `input_numeric_only`.\n",
        "\n",
        "2. **Feature Vector Assembly**: It uses `VectorAssembler` to assemble the feature columns of `input_numeric_only` (excluding the last column, assumed to be the label column) into a single feature vector column called `\"featureVector\"`.\n",
        "\n",
        "3. **Standard Scaling**: It standardizes the feature vector column using `StandardScaler`, creating a new column `\"scaledFeatureVector\"`.\n",
        "\n",
        "4. **KMeans Model Creation**: It creates a KMeans model with a randomly generated seed, a specified number of clusters `k`, maximum iterations of 40, tolerance of 1.0e-5, using the scaled feature vector column `\"scaledFeatureVector\"` for clustering, and setting the prediction column name to `\"cluster\"`.\n",
        "\n",
        "5. **Pipeline Creation and Fitting**: It creates a pipeline with the `VectorAssembler`, `StandardScaler`, and KMeans model, and fits the pipeline to the `input_numeric_only` DataFrame to train the KMeans model.\n",
        "\n",
        "6. **Training Cost Extraction**: It extracts the trained KMeans model from the pipeline model and retrieves the training cost (sum of squared distances of points to their nearest cluster center) from the model's summary.\n",
        "\n",
        "7. **Return Value**: It returns the training cost of the KMeans model.\n",
        "\n",
        "Overall, the function `clustering_score_2` is an extension of `clustering_score`, adding standard scaling of features before KMeans clustering, potentially improving clustering performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiUqULYBe8ai"
      },
      "outputs": [],
      "source": [
        "def clustering_score_2(input_data, k):\n",
        "    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
        "    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
        "    scaler = StandardScaler().setInputCol(\"featureVector\").setOutputCol(\"scaledFeatureVector\").setWithStd(True).setWithMean(False)\n",
        "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\")\n",
        "    pipeline = Pipeline().setStages([assembler, scaler, kmeans])\n",
        "    pipeline_model = pipeline.fit(input_numeric_only)\n",
        "    kmeans_model = pipeline_model.stages[-1]\n",
        "    training_cost = kmeans_model.summary.trainingCost\n",
        "    return training_cost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBvdwrAwe8ai"
      },
      "source": [
        "#### Printing the score for each k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zED9oLte8ai",
        "outputId": "8fd12be3-84e2-42a0-d78c-09768beb9b59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60 594473.2681887054\n",
            "90 408499.30381192255\n",
            "120 243875.62350682\n",
            "150 185417.2358386817\n",
            "180 151347.1862905078\n",
            "210 126555.87022985023\n",
            "240 111203.3118755722\n",
            "270 99073.10534575064\n"
          ]
        }
      ],
      "source": [
        "for k in list(range(60, 271, 30)):\n",
        "    print(k, clustering_score_2(numeric_only, k))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwYALB2Je8aj"
      },
      "source": [
        "### Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Pa3CByze8aj"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWiKFH5me8aj"
      },
      "source": [
        "## Function Summary: `one_hot_pipeline`\n",
        "\n",
        "1. **String Indexing**: It creates a `StringIndexer` to index the input column `input_col`, creating a new indexed column named `input_col + \"_indexed\"`.\n",
        "\n",
        "2. **One-Hot Encoding**: It creates a `OneHotEncoder` to encode the indexed column `input_col + \"_indexed\"`, creating a new one-hot encoded column named `input_col + \"_vec\"`.\n",
        "\n",
        "3. **Pipeline Creation**: It creates a pipeline with the `StringIndexer` and `OneHotEncoder` stages.\n",
        "\n",
        "4. **Return Value**: It returns the pipeline and the name of the one-hot encoded column (`input_col + \"_vec\"`).\n",
        "\n",
        "Overall, the `one_hot_pipeline` function encapsulates the process of indexing and one-hot encoding a single input column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mpXEsU0e8ak"
      },
      "outputs": [],
      "source": [
        "def one_hot_pipeline(input_col):\n",
        "    indexer = StringIndexer().setInputCol(input_col).setOutputCol(input_col +\"_indexed\")\n",
        "    encoder = OneHotEncoder().setInputCol(input_col + \"_indexed\").setOutputCol(input_col + \"_vec\")\n",
        "    pipeline = Pipeline().setStages([indexer, encoder])\n",
        "    return pipeline, input_col + \"_vec\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WQqNZnxe8ak"
      },
      "source": [
        "## Function Summary: `clustering_score_3`\n",
        "\n",
        "1. **One-Hot Encoding Pipelines**:\n",
        "   - Utilizes the `one_hot_pipeline` function to create one-hot encoding pipelines for the columns `\"protocol_type\"`, `\"service\"`, and `\"flag\"`.\n",
        "   - Extracts the resulting one-hot encoded columns for each feature.\n",
        "\n",
        "2. **Feature Assembly**:\n",
        "   - Determines the feature columns for clustering by excluding `\"label\"`, `\"protocol_type\"`, `\"service\"`, and `\"flag\"` columns.\n",
        "   - Includes the one-hot encoded columns in the feature columns.\n",
        "\n",
        "3. **Vector Assembler**:\n",
        "   - Uses a `VectorAssembler` to assemble the selected feature columns into a single feature vector column named `\"featureVector\"`.\n",
        "\n",
        "4. **Standard Scaling**:\n",
        "   - Uses a `StandardScaler` to standardize the feature vector column, producing a new column named `\"scaledFeatureVector\"`.\n",
        "\n",
        "5. **KMeans Model Creation**:\n",
        "   - Creates a `KMeans` model with parameters such as the number of clusters `k`, maximum iterations, and tolerance.\n",
        "   - Sets the seed for reproducibility.\n",
        "\n",
        "6. **Creating Pipeline**:\n",
        "   - Creates a `Pipeline` with stages including the one-hot encoding pipelines, `VectorAssembler`, `StandardScaler`, and `KMeans` stages.\n",
        "\n",
        "7. **Fitting Pipeline**:\n",
        "   - Fits the pipeline to the input data to create a `PipelineModel`.\n",
        "\n",
        "8. **Extracting Training Cost**:\n",
        "   - Extracts the trained KMeans model from the pipeline model and retrieves the training cost.\n",
        "\n",
        "9. **Return Value**:\n",
        "   - Returns the training cost of the KMeans model.\n",
        "\n",
        "10. **Overall Purpose**:\n",
        "    - The function `clustering_score_3` is used to train a KMeans clustering model with one-hot encoding for categorical features and evaluate its training cost.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xRXCDc1e8ak"
      },
      "outputs": [],
      "source": [
        "def clustering_score_3(input_data, k):\n",
        "    proto_type_pipeline, proto_type_vec_col = one_hot_pipeline(\"protocol_type\")\n",
        "    service_pipeline, service_vec_col = one_hot_pipeline(\"service\")\n",
        "    flag_pipeline, flag_vec_col = one_hot_pipeline(\"flag\")\n",
        "    assemble_cols = set(input_data.columns) - {\"label\", \"protocol_type\", \"service\", \"flag\"} | {proto_type_vec_col, service_vec_col, flag_vec_col}\n",
        "    assembler = VectorAssembler().setInputCols(list(assemble_cols)).setOutputCol(\"featureVector\")\n",
        "    scaler = StandardScaler().setInputCol(\"featureVector\").setOutputCol(\"scaledFeatureVector\").setWithStd(True).setWithMean(False)\n",
        "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\")\n",
        "    pipeline = Pipeline().setStages([proto_type_pipeline, service_pipeline,flag_pipeline, assembler, scaler, kmeans])\n",
        "    pipeline_model = pipeline.fit(input_data)\n",
        "    kmeans_model = pipeline_model.stages[-1]\n",
        "    training_cost = kmeans_model.summary.trainingCost\n",
        "    return training_cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqM7xcgJe8al",
        "outputId": "63deb8e5-a0dd-4fc2-fe8c-a8e5e19ecbc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60 17388727.09984871\n",
            "90 6394617.506449859\n",
            "120 1519827.0375092942\n",
            "150 999082.8065718799\n",
            "180 796774.6873166809\n",
            "210 582344.6975851612\n",
            "240 475594.9717839229\n",
            "270 385256.792086577\n"
          ]
        }
      ],
      "source": [
        "for k in list(range(60, 271, 30)):\n",
        "    print(k, clustering_score_3(data, k))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01HV0FY_e8al"
      },
      "source": [
        "### Using Labels with Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFAlYxqxe8al"
      },
      "outputs": [],
      "source": [
        "from math import log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP-SLYtBe8am"
      },
      "source": [
        "## Function Summary: `entropy`\n",
        "\n",
        "1. **Calculating Non-Zero Counts**: It filters out counts `c` from the input `counts` where `c` is greater than 0 and stores them in `values`.\n",
        "\n",
        "2. **Total Count Calculation**: It calculates the total count `n` by summing up all values in `values`.\n",
        "\n",
        "3. **Probability Calculation**: It calculates the probability `p` for each non-zero count `v` in `values` as `v/n`.\n",
        "\n",
        "4. **Entropy Calculation**: It calculates the entropy as the sum of the negative of each probability times the logarithm of the probability, i.e., `sum([-1*(p_v) * log(p_v) for p_v in p])`.\n",
        "\n",
        "5. **Return Value**: It returns the calculated entropy value.\n",
        "\n",
        "Overall, the `entropy` function computes the entropy of a distribution represented by the input counts, which is a measure of uncertainty or disorder in the distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrMCtfY3e8am"
      },
      "outputs": [],
      "source": [
        "def entropy(counts):\n",
        "    values = [c for c in counts if (c > 0)]\n",
        "    n = sum(values)\n",
        "    p = [v/n for v in values]\n",
        "    return sum([-1*(p_v) * log(p_v) for p_v in p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmJwHSmte8am"
      },
      "source": [
        "## Import Summary\n",
        "\n",
        "- `functions` module from `pyspark.sql` as `fun`: Used to access SQL functions in PySpark for DataFrame operations.\n",
        "- `Window` class from `pyspark.sql`: Used to define window specifications for window functions in PySpark DataFrame operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSYHRAjoe8an"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as fun\n",
        "from pyspark.sql import Window\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzDJ6E0ve8an"
      },
      "source": [
        "## 1. **Transforming Data**:\n",
        "- Use the fitted pipeline model (`pipeline_model`) to transform the input data `data` and select the \"cluster\" and \"label\" columns, creating a new DataFrame `cluster_label`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvogQ2ame8an"
      },
      "outputs": [],
      "source": [
        "cluster_label = pipeline_model.transform(data).select(\"cluster\", \"label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJtZjNBWe8ao"
      },
      "source": [
        "## 2. **Grouping and Counting**:\n",
        "- Group the `cluster_label` DataFrame by \"cluster\" and \"label\", count the occurrences of each cluster-label combination, and order the result by \"cluster\", creating a new DataFrame `df`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ezxwdt-e8ao"
      },
      "outputs": [],
      "source": [
        "df = cluster_label.groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV9LgO7Ne8ao"
      },
      "source": [
        "## 3. **Window Specification**:\n",
        "- Define a window specification `w` partitioned by \"cluster\" for use in window functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ENi-MiNe8ao"
      },
      "outputs": [],
      "source": [
        "w = Window.partitionBy(\"cluster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weHcw50Je8ap"
      },
      "source": [
        "## 4. **Calculating Probabilities**:\n",
        "- Calculate the probability `p_col` for each cluster-label combination as the count of the combination divided by the sum of counts for the cluster, using the `over` window function to sum counts within each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y0BTCHze8ap"
      },
      "outputs": [],
      "source": [
        "p_col = df['count'] / fun.sum(df['count']).over(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teO6hGAXe8ap"
      },
      "source": [
        "## 5. **Adding Probability Column**:\n",
        "- Add the calculated probabilities as a new column \"p_col\" to the DataFrame `df`, creating a new DataFrame `with_p_col`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWDoIEEne8aq"
      },
      "outputs": [],
      "source": [
        "with_p_col = df.withColumn(\"p_col\", p_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8jxCTWYe8aq"
      },
      "source": [
        "## 6. **Calculating Entropy**:\n",
        "- Calculate the entropy for each cluster by summing `-p_col * log2(p_col)` for each cluster-label combination, creating a new DataFrame `result`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C7zC4Hie8aq"
      },
      "outputs": [],
      "source": [
        "result = with_p_col.groupBy(\"cluster\").agg((-fun.sum(col(\"p_col\") * fun.log2(col(\"p_col\")))).alias(\"entropy\"),\n",
        "fun.sum(col(\"count\")).alias(\"cluster_size\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSF5d1uke8aq"
      },
      "source": [
        "## 7. **Calculating Weighted Cluster Entropy**:\n",
        "- Calculate the weighted cluster entropy by multiplying the entropy for each cluster by the cluster size (sum of counts) for that cluster, adding a new column \"weightedClusterEntropy\" to the `result` DataFrame.\n",
        "\n",
        "## 8. **Calculating Average**:\n",
        "- Calculate the average weighted cluster entropy by summing the weighted cluster entropies for all clusters and dividing by the total count of records in the input data `data`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GG9goxHe8ar",
        "outputId": "a9d13f0e-fd8e-4637-cdac-6dfe66c5809d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.557605039016584"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = result.withColumn('weightedClusterEntropy',fun.col('entropy') * fun.col('cluster_size'))\n",
        "weighted_cluster_entropy_avg = result.agg(fun.sum(col('weightedClusterEntropy'))).collect()\n",
        "weighted_cluster_entropy_avg[0][0]/data.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQZmAlXPe8ar"
      },
      "source": [
        "## Function Summary: `fit_pipeline_4`\n",
        "\n",
        "1. **One-Hot Encoding Pipelines**: It creates three one-hot encoding pipelines (`proto_type_pipeline`, `service_pipeline`, `flag_pipeline`) using the `one_hot_pipeline` function for the columns `\"protocol_type\"`, `\"service\"`, and `\"flag\"`, respectively. These pipelines produce one-hot encoded columns (`proto_type_vec_col`, `service_vec_col`, `flag_vec_col`).\n",
        "\n",
        "2. **Feature Assembly**: It assembles the feature columns for clustering, including the one-hot encoded columns, by taking the set of all input columns except `\"label\"`, `\"protocol_type\"`, `\"service\"`, and `\"flag\"` and adding the one-hot encoded columns.\n",
        "\n",
        "3. **Vector Assembler**: It creates a `VectorAssembler` to assemble the selected feature columns into a single feature vector column named `\"featureVector\"`.\n",
        "\n",
        "4. **Standard Scaling**: It standardizes the feature vector column using `StandardScaler`, creating a new column named `\"scaledFeatureVector\"`.\n",
        "\n",
        "5. **KMeans Model Creation**: It creates a KMeans model with a randomly generated seed, a specified number of clusters `k`, maximum iterations of 40, tolerance of 1.0e-5, using the scaled feature vector column `\"scaledFeatureVector\"` for clustering, and setting the prediction column name to `\"cluster\"`.\n",
        "\n",
        "6. **Pipeline Creation and Fitting**: It creates a pipeline with the one-hot encoding pipelines, `VectorAssembler`, `StandardScaler`, and KMeans model stages, and fits the pipeline to the input data `data`.\n",
        "\n",
        "7. **Return Value**: It returns the fitted pipeline, which can be used to transform new data.\n",
        "\n",
        "Overall, the `fit_pipeline_4` function creates a pipeline for clustering with one-hot encoding, feature assembly, standard scaling, and KMeans clustering, and fits the pipeline to the input data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b9njS9-e8ar"
      },
      "outputs": [],
      "source": [
        "def fit_pipeline_4(data, k):\n",
        "    (proto_type_pipeline, proto_type_vec_col) = one_hot_pipeline(\"protocol_type\")\n",
        "    (service_pipeline, service_vec_col) = one_hot_pipeline(\"service\")\n",
        "    (flag_pipeline, flag_vec_col) = one_hot_pipeline(\"flag\")\n",
        "    assemble_cols = set(data.columns) - {\"label\", \"protocol_type\", \"service\",\"flag\"} | {proto_type_vec_col, service_vec_col, flag_vec_col}\n",
        "    assembler = VectorAssembler(inputCols=list(assemble_cols),outputCol=\"featureVector\")\n",
        "    scaler = StandardScaler(inputCol=\"featureVector\",outputCol=\"scaledFeatureVector\", withStd=True, withMean=False)\n",
        "    kmeans = KMeans(seed=randint(100, 100000), k=k, predictionCol=\"cluster\",\n",
        "    featuresCol=\"scaledFeatureVector\", maxIter=40, tol=1.0e-5)\n",
        "    pipeline = Pipeline(stages=[proto_type_pipeline, service_pipeline,flag_pipeline, assembler, scaler, kmeans])\n",
        "    return pipeline.fit(data)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1adxAbk3e8as"
      },
      "source": [
        "## Function Summary: `clustering_score_4`\n",
        "\n",
        "1. **Pipeline Fitting**: It fits a pipeline (`fit_pipeline_4`) to the input data `input_data` with a specified number of clusters `k`, producing a pipeline model.\n",
        "\n",
        "2. **Cluster-Label DataFrame**: It transforms the input data using the fitted pipeline model to add a `\"cluster\"` column, then selects the `\"cluster\"` and `\"label\"` columns.\n",
        "\n",
        "3. **Grouping and Aggregation**: It groups the cluster-label DataFrame by `\"cluster\"` and `\"label\"`, counts the occurrences of each combination, and orders the result by `\"cluster\"`.\n",
        "\n",
        "4. **Window Function**: It defines a window function `w` partitioned by `\"cluster\"`.\n",
        "\n",
        "5. **Calculating Probabilities**: It calculates the probability `p_col` for each cluster-label combination as the count of the combination divided by the sum of counts for the cluster.\n",
        "\n",
        "6. **Adding Probability Column**: It adds the `\"p_col\"` column to the DataFrame with the probabilities.\n",
        "\n",
        "7. **Entropy Calculation**: It calculates the entropy for each cluster by summing `-p_col * log2(p_col)` for each cluster-label combination.\n",
        "\n",
        "8. **Weighted Cluster Entropy Calculation**: It calculates the weighted cluster entropy by multiplying the entropy for each cluster by the cluster size (sum of counts) for that cluster.\n",
        "\n",
        "9. **Average Weighted Cluster Entropy**: It calculates the average weighted cluster entropy by summing the weighted cluster entropies for all clusters and dividing by the total count of records in the input data `input_data`.\n",
        "\n",
        "10. **Return Value**: It returns the average weighted cluster entropy, which is a measure of the average uncertainty or disorder in the clustering results, weighted by the size of each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l2Lnzpje8as"
      },
      "outputs": [],
      "source": [
        "def clustering_score_4(input_data, k):\n",
        "    pipeline_model = fit_pipeline_4(input_data, k)\n",
        "    cluster_label = pipeline_model.transform(input_data).select(\"cluster\",\"label\")\n",
        "    df = cluster_label.groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\")\n",
        "    w = Window.partitionBy(\"cluster\")\n",
        "    p_col = df['count'] / fun.sum(df['count']).over(w)\n",
        "    with_p_col = df.withColumn(\"p_col\", p_col)\n",
        "    result = with_p_col.groupBy(\"cluster\").agg(-fun.sum(col(\"p_col\") * fun.log2(col(\"p_col\"))).alias(\"entropy\"), fun.sum(col(\"count\")).alias(\"cluster_size\"))\n",
        "    result = result.withColumn('weightedClusterEntropy', col('entropy') * col('cluster_size'))\n",
        "    weighted_cluster_entropy_avg = result.agg(fun.sum(col('weightedClusterEntropy'))).collect()\n",
        "    return weighted_cluster_entropy_avg[0][0] / input_data.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21tz6-h4e8at"
      },
      "source": [
        "## Clustering in Action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmJY6C6Ne8at"
      },
      "source": [
        "## 1. **Fitting Pipeline**:\n",
        "- Fit a pipeline model (`pipeline_model`) to the input data `data` with 180 clusters using the `fit_pipeline_4` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD4sK9LHe8at"
      },
      "outputs": [],
      "source": [
        "pipeline_model = fit_pipeline_4(data, 180)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1jJf_Fje8au"
      },
      "source": [
        "## 2. **Transforming Data and Counting**:\n",
        "- Transform the input data using the fitted pipeline model to add a \"cluster\" column.\n",
        "- Select the \"cluster\" and \"label\" columns, then group by \"cluster\" and \"label\".\n",
        "- Count the occurrences of each cluster-label combination and order the result by \"cluster\" and \"label\".\n",
        "- Display the result using the `show()` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wUZps3Ne8au",
        "outputId": "fedf0d4c-5d76-4122-b3b0-9bf0a336d565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------+-----+\n",
            "|cluster|      label|count|\n",
            "+-------+-----------+-----+\n",
            "|      0|   neptune.|36459|\n",
            "|      0| portsweep.|    9|\n",
            "|      1|      back.|    1|\n",
            "|      1|    normal.| 6029|\n",
            "|      2|   neptune.|   94|\n",
            "|      2|     satan.|    1|\n",
            "|      3|   neptune.|   80|\n",
            "|      4|   neptune.|  107|\n",
            "|      4| portsweep.|    1|\n",
            "|      5|loadmodule.|    2|\n",
            "|      5|  multihop.|    1|\n",
            "|      6|   neptune.|  177|\n",
            "|      6|    normal.|    1|\n",
            "|      6| portsweep.|    1|\n",
            "|      7|   neptune.|   24|\n",
            "|      7| portsweep.|    4|\n",
            "|      7|     satan.|    1|\n",
            "|      8|   neptune.|   77|\n",
            "|      9|   neptune.|   90|\n",
            "|      9|    normal.|    1|\n",
            "+-------+-----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "count_by_cluster_label = pipeline_model.transform(data).select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\", \"label\")\n",
        "count_by_cluster_label.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrGVLZaVe8av"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
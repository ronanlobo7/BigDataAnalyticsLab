{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Recommendation System with ALS Algorithm\n",
        "\n",
        "This Jupyter notebook demonstrates building a recommendation system using PySpark, with the ALS (Alternating Least Squares) algorithm. ALS is a collaborative filtering algorithm commonly used for recommendation systems.\n",
        "\n",
        "## Introduction to Recommendation Systems\n",
        "\n",
        "Recommendation systems are crucial for providing personalized recommendations to users based on their preferences and past interactions. They analyze user-item interactions to generate recommendations.\n"
      ],
      "metadata": {
        "id": "yY1ZvVGLFraf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Displaying Ratings Data\n",
        "\n",
        "We initialize a Spark session using `SparkSession.builder.getOrCreate()` and load ratings data from a JSON file named \"movies 1.json\" into a DataFrame named `ratings`. We select columns \"user_id\", \"product_id\", and \"score\" from the dataset, caching it for efficiency. Subsequently, we limit the DataFrame to the first 10,000 rows using `ratings.head(10000)` and create a new DataFrame from this limited data. Finally, we display the first 5 rows of the `ratings` DataFrame.\n"
      ],
      "metadata": {
        "id": "64HB6j9NETuu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8d7-b8CEAkS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "ratings = spark.read.json(\"movies 1.json\").select(\"user_id\",\"product_id\",\"score\").cache()\n",
        "ratings = ratings.head(10000)\n",
        "ratings = spark.createDataFrame(ratings)\n",
        "\n",
        "ratings.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collaborative Filtering with ALS Algorithm\n",
        "\n",
        "We import necessary modules for collaborative filtering using the ALS algorithm, including `ALS` from `pyspark.ml.recommendation` and `StringIndexer` from `pyspark.ml.feature`. We create `StringIndexer` instances for columns \"user_id\" and \"product_id\" to convert them into numerical indices. Then, we construct a `Pipeline` to apply indexers sequentially to the `ratings` DataFrame. After indexing, we split the data into training and validation sets using `randomSplit()`. We initialize the ALS model with specified parameters and fit it to the training data. Finally, we make predictions on the validation data and display the first 10 predictions.\n"
      ],
      "metadata": {
        "id": "dDtg0a08EUlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(ratings)\n",
        "    for column in [\"user_id\", \"product_id\"]\n",
        "]\n",
        "\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "ratings_indexed = pipeline.fit(ratings).transform(ratings)\n",
        "\n",
        "training_data,validation_data = ratings_indexed.randomSplit([8.0,2.0])\n",
        "\n",
        "als = ALS(userCol=\"user_id_index\",itemCol=\"product_id_index\",ratingCol=\"score\",rank=10,maxIter=5,regParam=0.01,coldStartStrategy=\"drop\")\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\",labelCol=\"score\",predictionCol=\"prediction\")\n",
        "\n",
        "model = als.fit(training_data)\n",
        "predictions=model.transform(validation_data)\n",
        "predictions.show(10,False)"
      ],
      "metadata": {
        "id": "TEFgeQPkEUw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Recommendations for a Specific User\n",
        "\n",
        "We filter the validation data to extract information for a specific user (with index 1.0 in this case), selecting relevant columns including 'product_id', 'user_id', 'user_id_index', and 'product_id_index'. We display the data for this user using `user1.show()`. Then, we use the trained ALS model to generate recommendations for this user by applying `model.transform()` on `user1`. Finally, we sort the recommendations by prediction scores in descending order using `orderBy('prediction', ascending=False)` and display the results.\n"
      ],
      "metadata": {
        "id": "sFHr79EEEU8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user1 = validation_data.filter(validation_data['user_id_index']==1.0).select(['product_id','user_id','user_id_index','product_id_index'])\n",
        "user1.show()\n",
        "recommendations = model.transform(user1)\n",
        "recommendations.orderBy('prediction',ascending=False).show()"
      ],
      "metadata": {
        "id": "-6cUT6bKEVIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Model Performance\n",
        "\n",
        "We use a `RegressionEvaluator` to compute the Root Mean Squared Error (RMSE) between actual and predicted ratings (`predictions`). The RMSE is calculated and printed using `evaluator.evaluate(predictions)`. Additionally, we create another evaluator for Mean Absolute Error (MAE), compute the MAE between actual and predicted ratings, and print the result using `evaluator_mae.evaluate(predictions)`.\n"
      ],
      "metadata": {
        "id": "6jEtoEHYEVT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Squared Error (RMSE) = {rmse}\")\n",
        "\n",
        "evaluator_mae = RegressionEvaluator(\n",
        "    metricName=\"mae\",\n",
        "    labelCol=\"score\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "\n",
        "mae = evaluator_mae.evaluate(predictions)\n",
        "print(f\"Mean Absolute Error (MAE) = {mae}\")"
      ],
      "metadata": {
        "id": "6BFdAydZEVf6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}